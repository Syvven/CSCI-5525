{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Train Data\n",
      "Emotion: angry, File Count: 3995\n",
      "Emotion: disgust, File Count: 436\n",
      "Emotion: fear, File Count: 4097\n",
      "Emotion: happy, File Count: 7215\n",
      "Emotion: neutral, File Count: 4965\n",
      "Emotion: sad, File Count: 4830\n",
      "Emotion: surprise, File Count: 3171\n",
      "\n",
      "Extracting Test Data\n",
      "Emotion: angry, File Count: 958\n",
      "Emotion: disgust, File Count: 111\n",
      "Emotion: fear, File Count: 1024\n",
      "Emotion: happy, File Count: 1774\n",
      "Emotion: neutral, File Count: 1233\n",
      "Emotion: sad, File Count: 1247\n",
      "Emotion: surprise, File Count: 831\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "###################################################################\n",
    "# This data preprocess does not assume 48x48 grayscale images.\n",
    "# An image passed through it will be turned gray and scaled to \n",
    "#  48x48 pixels then turned into a pytorch tensor.\n",
    "###################################################################\n",
    "\n",
    "label_map = {\n",
    "    'angry': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'happy': 3,\n",
    "    'neutral': 4,\n",
    "    'sad': 5,\n",
    "    'surprise': 6\n",
    "}\n",
    "\n",
    "predict_map = {\n",
    "    0: 'angry',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happy',\n",
    "    4: 'neutral',\n",
    "    5: 'sad',\n",
    "    6: 'surprise'\n",
    "}\n",
    "\n",
    "directory = './data/train'\n",
    "\n",
    "width = 48\n",
    "\n",
    "file_count = 0\n",
    "for dir in os.listdir(directory):\n",
    "    curr_dir = os.path.join(directory, dir)\n",
    "    file_count += len(os.listdir(curr_dir))\n",
    "\n",
    "train_labels = torch.zeros((file_count, 1, 7))\n",
    "train_data = torch.zeros((file_count, 1, width, width))\n",
    "\n",
    "curr_data_point1 = 0\n",
    "last = 0\n",
    "print(\"Extracting Train Data\")\n",
    "convert_tensor = transforms.ToTensor()\n",
    "for dir in os.listdir(directory):\n",
    "    curr_dir = os.path.join(directory, dir)\n",
    "    for filename in os.listdir(curr_dir):\n",
    "      f = os.path.join(curr_dir, filename)\n",
    "      if os.path.isfile(f):\n",
    "          img = ImageOps.grayscale(Image.open(f)).resize((width, width))\n",
    "          train_data[curr_data_point1][0] = convert_tensor(img)\n",
    "          train_labels[curr_data_point1][0][label_map[dir]] = 1\n",
    "          curr_data_point1 += 1\n",
    "    print(f\"Emotion: {dir}, File Count: {curr_data_point1-last}\")\n",
    "    last = curr_data_point1\n",
    "\n",
    "print()\n",
    "print(\"Extracting Test Data\")\n",
    "\n",
    "directory = './data/test'\n",
    "\n",
    "file_count = 0\n",
    "for dir in os.listdir(directory):\n",
    "    curr_dir = os.path.join(directory, dir)\n",
    "    file_count += len(os.listdir(curr_dir))\n",
    "\n",
    "test_labels = torch.zeros((file_count, 1, 7))\n",
    "test_data = torch.zeros((file_count, 1, width, width))\n",
    "curr_data_point2 = 0\n",
    "last = 0\n",
    "for dir in os.listdir(directory):\n",
    "    curr_dir = os.path.join(directory, dir)\n",
    "    for filename in os.listdir(curr_dir):\n",
    "      f = os.path.join(curr_dir, filename)\n",
    "      if os.path.isfile(f):\n",
    "          img = ImageOps.grayscale(Image.open(f)).resize((width, width))\n",
    "          test_data[curr_data_point2][0] = convert_tensor(img)\n",
    "          test_labels[curr_data_point2][0][label_map[dir]] = 1\n",
    "          curr_data_point2 += 1\n",
    "    print(f\"Emotion: {dir}, File Count: {curr_data_point2-last}\")\n",
    "    last = curr_data_point2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, channel_in, pass_on, channel_out, device):\n",
    "        super().__init__()\n",
    "        self.conv_1x1 = nn.Sequential(\n",
    "            nn.Conv2d(channel_in, channel_out[\"1x1\"], kernel_size=1),\n",
    "            nn.BatchNorm2d(channel_out[\"1x1\"]),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "    \n",
    "        # 3x3 branch, we padding 1 in the 3x3 convolution layer to keep same size of image\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(channel_in, pass_on[\"3x3\"], kernel_size=1),\n",
    "            nn.BatchNorm2d(pass_on[\"3x3\"]),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(pass_on[\"3x3\"], channel_out[\"3x3\"], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channel_out[\"3x3\"]),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # 5x5 branch, we padding 2 in the 5x5 convolution layer to keep same size of image\n",
    "        self.conv_5x5 = nn.Sequential(\n",
    "            nn.Conv2d(channel_in, pass_on[\"5x5\"], kernel_size=1),\n",
    "            nn.BatchNorm2d(pass_on[\"5x5\"]),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(pass_on[\"5x5\"], channel_out[\"5x5\"], kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(channel_out[\"5x5\"]),\n",
    "            nn.PReLU()\n",
    "        ) \n",
    "        # Max pooling branch\n",
    "        self.max_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
    "            nn.Conv2d(channel_in, channel_out[\"max\"], kernel_size=1),\n",
    "            nn.BatchNorm2d(channel_out[\"max\"]),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [\n",
    "                self.conv_1x1(x), self.conv_3x3(x),\n",
    "                self.conv_5x5(x), self.max_pool(x)\n",
    "            ], dim=1 # concatenate along channels\n",
    "        )\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNET(nn.Module):\n",
    "    def __init__(self, n_classes, epochs, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            # input layer\n",
    "            nn.Conv2d(1, 64, kernel_size=7, padding=3),\n",
    "            nn.PReLU(),\n",
    "            nn.LocalResponseNorm(128),\n",
    "            nn.Conv2d(64, 112, kernel_size=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(112, 196, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.LocalResponseNorm(128),\n",
    "            # pass through blocks\n",
    "            Block(\n",
    "                196, \n",
    "                pass_on={\"3x3\": 96, \"5x5\": 16}, \n",
    "                channel_out={\"1x1\": 64, \"3x3\": 128, \"5x5\": 32, \"max\": 32},\n",
    "                device=device\n",
    "            ),\n",
    "            Block(\n",
    "                256, \n",
    "                pass_on={\"3x3\": 128, \"5x5\": 32}, \n",
    "                channel_out={\"1x1\": 128, \"3x3\": 192, \"5x5\": 96, \"max\": 64},\n",
    "                device=device\n",
    "            ),\n",
    "            # reduce dimensions\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),  \n",
    "            # pass through blocks\n",
    "            Block(\n",
    "                480, \n",
    "                pass_on={\"3x3\": 96, \"5x5\": 16}, \n",
    "                channel_out={\"1x1\": 192, \"3x3\": 208, \"5x5\": 48, \"max\": 64}, \n",
    "                device=device\n",
    "            ),\n",
    "            Block(\n",
    "                512, \n",
    "                pass_on={\"3x3\": 112, \"5x5\": 24}, \n",
    "                channel_out={\"1x1\": 176, \"3x3\": 224, \"5x5\": 64, \"max\": 64}, \n",
    "                device=device\n",
    "            ),\n",
    "            # reduce dimensions\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            # pass through last blocks\n",
    "            Block(\n",
    "                528, \n",
    "                pass_on={\"3x3\": 160, \"5x5\": 32}, \n",
    "                channel_out={\"1x1\": 256, \"3x3\": 320, \"5x5\": 128, \"max\": 128},\n",
    "                device=device\n",
    "            ),\n",
    "            # pool\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            # classification head\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(832, n_classes),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.augment = transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.seq(X)\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        batch_size = 50\n",
    "        epoch_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        state_dic = None\n",
    "        min_val_loss = 9e15\n",
    "        break_count = 0\n",
    "        max_break_count = 10\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0\n",
    "            indices = torch.randperm(X_train.shape[0])\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "            \n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x = self.augment(X_train[i:i+batch_size]).to(self.device)\n",
    "                y = y_train[i:i+batch_size,0,:].to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                pred = self.forward(x)\n",
    "\n",
    "                loss = self.criterion(pred, y) \n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            epoch_losses.append(train_loss)\n",
    "\n",
    "            acc = 0\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(X_test), batch_size):\n",
    "                    x = X_test[i:i+batch_size].to(self.device)\n",
    "                    y = y_test[i:i+batch_size,0,:].to(self.device)\n",
    "                    \n",
    "                    preds = self.forward(x)\n",
    "\n",
    "                    valid_loss += self.criterion(preds, y).item()\n",
    "\n",
    "                    acc += torch.sum(\n",
    "                        torch.argmax(preds, dim=1) == torch.argmax(y, dim=1)\n",
    "                    )\n",
    "                \n",
    "            validation_losses.append(valid_loss)\n",
    "\n",
    "            if (valid_loss < min_val_loss):\n",
    "                min_val_loss = valid_loss\n",
    "                break_count = 0\n",
    "                state_dict = copy.deepcopy(self.state_dict())\n",
    "            \n",
    "            print(f\"[{epoch + 1}]\")\n",
    "            print(f\"   Training loss:       {train_loss}\")\n",
    "            print(f\"   Validation Acc:      {acc / X_test.shape[0]}\")\n",
    "            print(f\"   Validation Loss:     {valid_loss}\")\n",
    "            print(f\"   Min Validation Loss: {min_val_loss}\")\n",
    "\n",
    "            if (valid_loss > min_val_loss):\n",
    "                break_count += 1\n",
    "                if (break_count >= max_break_count):\n",
    "                    self.load_state_dict(state_dict)\n",
    "                    print(f\"Validation loss not improved in {break_count} epochs.\")\n",
    "                    print(f\"Ending Training Early.\")\n",
    "                    break\n",
    "        \n",
    "        return epoch_losses, validation_losses\n",
    "\n",
    "    def predict(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "   Training loss:       1079.1714961528778\n",
      "   Validation Acc:      0.20325997471809387\n",
      "   Validation Loss:     277.0945565700531\n",
      "   Min Validation Loss: 277.0945565700531\n",
      "[2]\n",
      "   Training loss:       1043.564003109932\n",
      "   Validation Acc:      0.22373922169208527\n",
      "   Validation Loss:     276.0327534675598\n",
      "   Min Validation Loss: 276.0327534675598\n",
      "[3]\n",
      "   Training loss:       1018.0593881607056\n",
      "   Validation Acc:      0.21955977380275726\n",
      "   Validation Loss:     276.81475353240967\n",
      "   Min Validation Loss: 276.0327534675598\n",
      "[4]\n",
      "   Training loss:       1000.4273828268051\n",
      "   Validation Acc:      0.22875453531742096\n",
      "   Validation Loss:     275.6122786998749\n",
      "   Min Validation Loss: 275.6122786998749\n",
      "[5]\n",
      "   Training loss:       985.389434337616\n",
      "   Validation Acc:      0.22722208499908447\n",
      "   Validation Loss:     275.90408170223236\n",
      "   Min Validation Loss: 275.6122786998749\n",
      "[6]\n",
      "   Training loss:       975.9644130468369\n",
      "   Validation Acc:      0.23223739862442017\n",
      "   Validation Loss:     275.5413269996643\n",
      "   Min Validation Loss: 275.5413269996643\n",
      "[7]\n",
      "   Training loss:       965.6316156387329\n",
      "   Validation Acc:      0.21677348017692566\n",
      "   Validation Loss:     277.4632933139801\n",
      "   Min Validation Loss: 275.5413269996643\n",
      "[8]\n",
      "   Training loss:       963.1720576286316\n",
      "   Validation Acc:      0.22694344818592072\n",
      "   Validation Loss:     276.16932332515717\n",
      "   Min Validation Loss: 275.5413269996643\n",
      "[9]\n",
      "   Training loss:       956.6638839244843\n",
      "   Validation Acc:      0.22652550041675568\n",
      "   Validation Loss:     276.6080343723297\n",
      "   Min Validation Loss: 275.5413269996643\n",
      "[10]\n",
      "   Training loss:       950.5048555135727\n",
      "   Validation Acc:      0.22819727659225464\n",
      "   Validation Loss:     276.14288425445557\n",
      "   Min Validation Loss: 275.5413269996643\n"
     ]
    }
   ],
   "source": [
    "model = LeNET(7, 10, DEVICE).to(DEVICE)\n",
    "\n",
    "e_loss, v_loss = model.train(train_data, train_labels, test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f5b707d86fd9281530b9fa2dbdbe1b33232c3b651a8e052360c651d4996094"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
